<!doctype html>
<html>
  <head>
    <!-- Global Site Tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-106869316-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments)};
      gtag('js', new Date());

      gtag('config', 'UA-106869316-1');
    </script>


    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Junting Pan's Homepage</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <link rel="stylesheet" href="stylesheets/bootstrap.min.css">
    <link rel="stylesheet" href="stylesheets/font-awesome.min.css">

    <link rel="stylesheet" href="stylesheets/special.css">

    
    <meta name="viewport" content="width=device-width">
    
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

  </head>
  <body>
    <div class="wrapper">
      <header>
        <br />
        <div align="center">
            <span class="image avatar" >
    		  <img src="./images/avatar.jpg" ></img>
            </span>
        </div>
        <h1><b>Junting Pan</b></h1>

        <p> 
            <a href="https://www.linkedin.com/in/junting-pan/"><font color="#222"><i class="fa fa-linkedin fa-2x"></i></font></a> &nbsp &nbsp
            <a href="https://github.com/junting"><font color="#222"><i class="fa fa-github fa-2x"></i> </font></a> &nbsp  &nbsp 
            <a href="https://scholar.google.com/citations?user=8Xt3TnAAAAAJ&hl=es"><font color="#222"><i class="fa fa-google fa-2x"></i> </font></a> &nbsp &nbsp

            <span class="cv_right">
                <a href="files/JuntingPan_CV.pdf"><font size="4em" color="#222"><b>[CV]</b></font></a>
            </span>
        </p>
        <br />

        <h4>Contact:</h4>
        <p>
            <font face="courier">junting.pa@gmail.com </font>
        </p>

        
      </header>


      <section class="outer">
      		<!-- About Me -->
      		<section  id="about">
	    	    <h2>About Me</h2>
	    			<p>I am currently a Ph.D.student in <a href="http://mmlab.ie.cuhk.edu.hk/">Multimedia Lab (MMLab)</a>, the Chinese University of Hong Kong. I am supervised by <a href="http://www.ee.cuhk.edu.hk/~xgwang/">Prof. Xiaogang Wang</a> and <a href="http://www.ee.cuhk.edu.hk/~hsli/">Prof. Hongsheng Li</a>. Before that,I received my Bachelor's degree in Electric Engineering from Universitat Politecnica de Calunya, BarcelonaTech. My Chinese name is 潘俊廷.</p>

	                <p>
	                    I worked in <a href="https://imatge.upc.edu/web/">Image Processing Group (GPI)</a>, advised by Prof. <a href="https://imatge.upc.edu/web/people/xavier-giro">Xavier Giro</a> since Spring 2015 and I have also collaborated with Prof. <a href="http://www.eeng.dcu.ie/~mcguinne/">Kevin McGuiness</a> from <a href="https://www.dcu.ie/">Dublin City University (DCU)</a> and Dr. <a href="https://cristiancanton.github.io/">Cristian Canton</a> from Facebook.
	                </p>

                    <p>
                         I visited <a href="http://www.ee.columbia.edu/ln/dvmm/">Digital Video and MultiMedia Lab (DVMM Lab)</a>, advised by Prof. <a href="http://www.ee.columbia.edu/~sfchang/">Shih-Fu Chang</a>. 
                    </p>
	                <p>My research interests lie in computer vision, deep learning, and their applications. Specifically on tasks related to Video understanding, Video Generation, and Scene Understanding.</p>
			</section>
			
			<!-- Education -->
			<section  id="education">
	            <h2>Education</h2>

                 <div class="media">
                    <span class="pull-left"><img src="./images/cuhk.png" width="98px" height="70px"/></span>
                    <div class="media-body">
                        <p>Multi-Media Laboraory, <i><b>the Chinese University of Hong Kong, Hong Kong</b></i>,</p>
                        <p>Ph.D. Working on Action Recognition, Video Understanding and Generation.</p>
                    </div>  
                </div>

	            <div class="media">
	                <span class="pull-left"><img src="./images/upc.png" width="80px" height="80"/></span>
	                <div class="media-body">
	                    <p>Department of Electric Engineering, <i><b>Universitat Politecnica de Calunya, BarcelonaTech</b></i>,</p>
	                    <p>Master of Engineering.</p>
	                </div>  
	            </div>

	            <div class="media">
	                <span class="pull-left"><img src="./images/columbia.jpg" width="96px" height="96px"/></span>
	                <div class="media-body">
	                    <p>DVMM Lab, Department of Computer Sciences, <i><b>Columbia University, New York</b></i>,</p>
	                    <p>Visiting Scholar.</p>
	                </div>
	            </div>
        	</section>
			
			<!-- Publications -->
			<section  id="publication">
	            <h2>Publications</h2>
	            <ul>                    

                    <li><b> <u><a href="https://arxiv.org/pdf/2306.11732.pdf"> Retrieving-to-Answer: Zero-Shot Video Question Answering with Frozen Large Language Models</a></u>, </b><i>technical report</i>
                    <!-- </a> <a href="https://github.com/linziyi96/st-adapter">[code]</a> -->
                    <i><font color="crimson"> New! </font></i> <br>
                    <b>J. Pan</b>, Z. Lin, Y. Ge, X. Zhu, R. Zhang, Y. Wang, Y. Qiao, H. Li &nbsp<br>
                    <font color="crimson"> SOTA videoQA with zero training. </font> <br><br>
                    </li>


                    <li><b> <u><a href="https://arxiv.org/pdf/2206.13559.pdf"> ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning</a></u>, </b><i>in NeurIPS 2022</i>
                    </a> <a href="https://github.com/linziyi96/st-adapter">[code]</a>
                    <!-- <i><font color="crimson"> New! </font></i> <br> --> 
                    <br>
                    <b>J. Pan</b>, Z. Lin, X. Zhu, J. Shao, H. Li &nbsp<br>
                    <font color="crimson"> Maybe the current fastest way to get SOTA models for video recognition. </font> <br><br>
                    </li>


                    <li><b> <u><a href="https://arxiv.org/pdf/2205.03436.pdf"> EdgeViTs: Competing Light-weight CNNs on Mobile Devices with Vision Transformers</a></u>, </b><i>in ECCV 2022</i>
                    </a> <a href="https://github.com/saic-fi/edgevit">[code]</a>
                    <!-- <i><font color="crimson"> New! </font></i> <br> -->
                    <br>
                    <b>J. Pan</b>, A. Bulat, F. Tan, X. Zhu, L. Dudziak, H. Li, G. Tzimiropoulos, B. Martinez &nbsp<br>
                    <!-- <font color="crimson"> SOTA Light weight ViTs.</font> <br> -->
                    <br>
                    </li>

                    <li><b> <u><a href="https://arxiv.org/pdf/2006.07976.pdf"> Actor-Context-Actor Relation Network forSpatio-Temporal Action Localization</a></u>, </b><i>in CVPR 2021</i>  
                    </a> <a href="https://github.com/Siyu-C/ACAR-Net">[code]</a> <br>
                    <b>J. Pan</b>*, S. Chen*, J. Shao, Z. Shou, H. Li &nbsp  <br>
                    <font color="crimson"> 1st place solution in ActivityNet AVA Challenge 2020 </font> <br><br>
                    </li>


                    <li><b> <u><a href="https://arxiv.org/pdf/1903.04480.pdf"> Video Generation from Single Semantic Label Map</a></u>, </b><i>in CVPR 2019</i> 
                    </a> <a href="https://github.com/junting/seg2vid">[code]</a><br>
                    <b>J. Pan</b>, C. Wang, X. Jia, J. Shao, L. Sheng, J. Yan, X. Wang &nbsp
                    <br><br>
                    </li>
                    <!-- <li><b>J. Pan</b>*, Z. Shou*, J. Chan, K. Miyazawa, H. Mansour, A. Vetro, X. Giro-i-Nieto, SF. Chang, Online detection of action start in untrimmed, streaming videos,
                    <i>in ECCV 2018</i>. &nbsp <a href="https://eccv2018.org/papers-at-cvf-open-access/">[pdf]</a> <a href="https://github.com/junting/odas">[code]</a></li> -->

                    <li><b> <u><a href="https://eccv2018.org/papers-at-cvf-open-access/"> Online detection of action start in untrimmed, streaming videos</a></u>, </b><i>in ECCV 2018</i><br>
                    <b>J. Pan</b>*, Z. Shou*, J. Chan, K. Miyazawa, H. Mansour, A. Vetro, X. Giro-i-Nieto, SF. Chang &nbsp</a> <br><br></li>
                    <!-- <li><b>J. Pan</b>, C. Canton, K. McGuinness, NE. O’Connor, J. Torres, E. Sayrol, X. Giro-i-Nieto, SalGAN: visual saliency prediction with adversarial networks,
	                <i>in CVPR SUNw, 2017 (<u>spotlight</u>)</i>. &nbsp <a href="https://github.com/imatge-upc/saliency-salgan-2017">[code]</a></li>  -->

                    <li><b><u> <a href="https://arxiv.org/pdf/1701.01081.pdf"> SalGAN: visual saliency prediction with adversarial networks</a></u>, </b><i>in CVPR SUNw, 2017</i>
                    <a href="https://github.com/imatge-upc/saliency-salgan-2017">[code]</a><br>
                    <b>J. Pan</b>, C. Canton, K. McGuinness, NE. O’Connor, J. Torres, E. Sayrol, X. Giro-i-Nieto &nbsp <br><br></li>
	                
                    <!-- <li><b>J. Pan</b>*, K. McGuinness*, NE. O’Connor, E. Sayrol, X. Giro-i-Nieto, Shallow and Deep Convolutional Networks for Saliency Prediction, <i>in CVPR 2016.</i> &nbsp <a href="https://imatge.upc.edu/web/sites/default/files/pub/cPan_0.pdf">[pdf]</a> <a href="https://github.com/imatge-upc/saliency-2016-cvpr">[code]</a></li> -->
                    <li><b><u> <a href="https://imatge.upc.edu/web/sites/default/files/pub/cPan_0.pdf"> Shallow and Deep Convolutional Networks for Saliency Prediction</a></u>, </b><i>in CVPR 2016</i> <a href="https://github.com/imatge-upc/saliency-2016-cvpr">[code]</a></li>
                    <b>J. Pan</b>*, K. McGuinness*, NE. O’Connor, E. Sayrol, X. Giro-i-Nieto &nbsp<br>

	            </ul>
        	</section>
                  
            
            <!-- Work -->
            <section  id="work">
                <h2>Professional Service</h2>
                <ul>
                    <li>Reviewer of CVPR 2019, 2020, 2021</li>
                    <li>Reviewer of ICCV 2019, 2021</li>
                    <li>Reviewer of ECCV 2020</li>
                    <li>Reviewer of IJCV, TPAMI, CVIU</li>
                    <li>Reviewer of ACM MM 2018</li>

                </ul>
	            <h2>Work Experience</h2>
	            <ul>
                    <li><strong><a href="https://ai.facebook.com/">FAIR, Meta AI</a></strong>
                        <br> <b>Reserach Scientist Intern </b>
                        <br> May 2023 - Now, San Francisco, US
                        <br>Working on multi-modal fundation models.</br> </li>
                    <li><strong><a href="https://research.samsung.com/aicenter_cambridge">Samsung Research AI Center, Cambridge</a></strong>
                        <br> <b>Reserach Intern </b>
                        <br> Sep 2021 - Mar 2022, Cambridge, UK
                        <br>Working on efficient mobile transformers.</br> </li>
	                <li><strong><a href="https://www.ee.columbia.edu/ln/dvmm/">DVMM Lab, Columbia University</a></strong> 
	                    <br> <b>Reserach Assistant </b>
	                    <br> Mar 2017 - Dec 2018, New York, US
	                    <br>Working on online action dectection.</br> </li>
	            </ul>
        	</section>

    	
    		<!-- Honors -->
            <section id="Honors">
                <h2>Honors and Awards</h2>
                    <ul>
                        <li><strong>1<sup>st</sup> Prize in ActivityNet Challenge 2020. (CVPR’20)</strong> &nbsp 2020
                            <br>AVA-Kinetics Crossover Spatio-temporal localization task.</br> </li>
                       <!--  <li><strong>1<sup>st</sup> Prize in Saliency Prediction task in LSUN Challenge. (CVPR’15)</strong> &nbsp 2015 -->
                            <!-- <br>Aim to predict salient region given a natural scene image.</br> </li> -->
                        <li><strong>Outstanding academic achievement recognition</strong>
                            <br>6.25% of all student</br> </li>
                        <li><strong>2<sup>nd</sup> Better Academic Record during the 2nd year of ICT degrees, UPC, ETSETB</strong> 
                            </li>
                        <li><strong>3<sup>rd</sup> Better Academic Record during the 2nd year of ICT degrees, UPC, ETSETB</strong>
                            </li>
                    </ul>
            </section>

            <section id='analytics'>
            <a href="https://clustrmaps.com/site/1ai3r" title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=fHKOjyAQM5OZpf7_YENiKNVtE2RzjQAC3yRTLPmVxuY&cl=ffffff"></a>
            </section>
 
        <!-- Footer -->
            
        </section>

        <section id="footer">
			<div class="container">
				<ul class="copyright">
					<li>&copy; Junting Pan 2018</li>
				</ul>
			</div>
		</section>
    <script src="javascripts/scale.fix.js"></script>
  </body>
</html>
