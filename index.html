<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Junting Pan</title>
  
  <meta name="author" content="Junting Pan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="image/icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Junting Pan</name>
              </p>
              <p>I am a final-year Ph.D.student in <a href="http://mmlab.ie.cuhk.edu.hk/">Multimedia Lab (MMLab)</a>, at the Chinese University of Hong Kong, supervised by <a href="http://www.ee.cuhk.edu.hk/~xgwang/">Prof. Xiaogang Wang</a> and <a href="http://www.ee.cuhk.edu.hk/~hsli/">Prof. Hongsheng Li</a>. 
                I have been fortunate to intern at <a href="https://ai.meta.com/blog/meta-fair-research-new-releases/">Meta FAIR</a> and <a href=" https://research.samsung.com/aicenter_cambridge">Samsung AI Center-Cambridge</a>.       

                <p>My research interests lie in computer vision, deep learning, and their applications with a particular interest in Video Understanding, Video Generation, and Multimodal Representation Learning.</p>
              
                <!-- <p>I obtained my Bachelor and Master's degress in Electric Engineering from Universitat Politecnica de Calunya, BarcelonaTech. </p>  -->
                <!-- <p> -->
                <!-- I received my Bachelor's and Master's degree in Electric Engineering from Universitat Politecnica de Calunya, BarcelonaTech. </p> -->
                <!-- I have been forunate to work/intern at Samsung AI Center (2021-2022). 
                Before that, I worked in <a href=3D"https://imatge.upc.edu/web/">Image Processing Group (GPI)</a>, advised by Prof. <a href=3D"https://imatge.=
                upc.edu/web/people/xavier-giro">Xavier Giro</a> (2015-2017), and I have also collaborated with Prof. <a href=3D"http://www.eeng.dcu.ie/~mcguinne/">Kevin McGuiness</a> from <a href=3D"https://www.dcu.ie/">Dublin City Univ=
                ersity (DCU)</a> and Dr. <a href=3D"https://cristiancanton.github.io/">Cristian Canton</a> from Facebook.
              </p> -->
                
              <!-- <p>I visited <a href=3D"http://www.ee.columbia.edu/ln/dvmm/">Digital Video and MultiMedia Lab (DVMM Lab)</a>, advised by Prof. <a href=3D"http://www.ee.columbia.edu/~sfchang/">Shih-Fu Chang</a>.</p> -->

              <p style="text-align:center">
                <a href="mailto:junting.pa@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/JuntingPan_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=8Xt3TnAAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/junting-pan/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/junting">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/avatar.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/avatar.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        <!-- </tbody></table> -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="0"><tbody>
          <style>
              @keyframes wave {
                  0% { transform: rotate(-10deg); }
                  50% { transform: rotate(10deg); }
                  100% { transform: rotate(-10deg); }
              }

              .waving {
                  display: inline-block;
                  animation: wave 1s infinite;
              }
         </style>
          <!-- 
          <tr>
          <td style="padding-left:20px;width:20%;vertical-align:middle">  06-2024 </td>
          <td>
            <span style="color: red;">New!</span>
            <a href="https://huggingface.co/datasets/MathLLMs/MathVision">Math-Vision</a> is accepted in NeurIPS 2024 Datasets and Benchmarks Track!</td>
          </tr> -->
          <tr>
              <td style="padding-left:20px;width:20%;vertical-align:middle">10-2024</td>
              <td>
                  <span style="color: red;" class="waving">New!</span>
                  <a href="https://huggingface.co/datasets/MathLLMs/MathVision">Math-Vision</a> is accepted in NeurIPS 2024 Datasets and Benchmarks Track!
              </td>
          </tr>
          <tr>
          <td style="padding-left:20px;width:20%;vertical-align:middle">  07-2024 </td>
          <td> <span style="color: red;" class="waving">New!</span> We release <a href="https://sam2.metademolab.com/">SAM 2</a>, a unified model for real-time, promptable video object segmentation.</td>
          </tr>
          <tr>
          <td style="padding-left:20px;width:20%;vertical-align:middle">  06-2024 </td>
          <td> We release <a href="https://huggingface.co/datasets/MathLLMs/MathVision">Math-Vision</a>, a benchmark for evaluating the mathematical reasoning abilities of LMMs.</td>
          </tr>
          <tr>
          <td style="padding-left:20px;width:20%;vertical-align:middle">  09-2023 </td>
          <td> <a href="https://journeydb.github.io/">JourneyDB</a> is accepted in NeurIPS 2023 Datasets and Benchmarks Track!</td>
          </tr>
          <tr>
          <td style="padding-left:20px;width:20%;vertical-align:middle">  07-2023 </td>
          <td> We release <a href="https://journeydb.github.io/">JourneyDB</a>, a large-scale benchmark for multimodal generative image understanding. </td>
          </tr>
	         <tr>
              <td style="padding-left:20px;width:20%;vertical-align:middle">   05-2023 </td>
            <td> Starting my internship as a research scientist Intern at Meta AI (FAIR).</td>
          </tr>
          <tr>
            <td style="padding-left:20px;width:20%;vertical-align:middle">   09-2022 </td>
          <td> Our paper <a href="https://arxiv.org/abs/2206.13559">ST-Adapter</a> on efficient image-to-video transfer learning is accepted to NeurIPS 2022. </td>
        </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;padding-top:40px;padding-bottom:0px;width:100%;vertical-align:middle">
            <heading>Work Experience</heading>
            <p>
            </p>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <td style="padding-left:20px;width:25%"> May 2023 - Nov 2023 </td>
        <td style="width:30%"> <strong><a href="https://ai.facebook.com/">FAIR, Meta AI,</a></strong></td>
      <td> Research Scientist Intern (Multi-Modal Fundation Models). </td>
    </tr>
    <tr>
      <td style="padding-left:20px;width:25%"> Sep 2021 - Mar 2022 </td>
      <td style="width:30%"> <strong><a href="https://research.samsung.com/aicenter_cambridge">Samsung Research AI Center</a></strong></td>
    <td> Research Intern (Efficient Mobile Transformers). </td>
  </tr>
  <tr>
    <td style="padding-left:20px;width:25%"> Mar 2017 - Dec 2017 </td>
    <td style="width:30%"> <strong><a href="https://www.ee.columbia.edu/ln/dvmm/">DVMM Lab, Columbia University</a></strong></td>
  <td> Research Assistant (Online Action Dectection). </td>
</tr>
  </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;padding-top:40px;padding-bottom:0px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
              </p>
            </td>
          </tr>
        </tbody></table>

         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;""><tbody>
          <tr onmouseout="figure_stop()" onmouseover="figure_start()">
            <td style="padding-left:20px;padding-top:00px;width:25%;vertical-align:middle">
              <div class="one">
              <video width="100%" playsinline="" autoplay="" muted="" loop="">
                <source src="images/sam2_output.mp4" type="video/mp4">
              </video>
              </div>
              <script type="text/javascript">
                function figure_start() {
                  document.getElementById('figure').style.opacity = "1";
                }

                function figure_stop() {
                  document.getElementById('figure').style.opacity = "0";
                }
                figure_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2408.00714">
                <papertitle>SAM 2: Segment Anything in Images and Videos
                </papertitle>
              </a>
              <br>
              Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr,
              Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, 
              <br>
              <b>Junting Pan</b>, Kalyan Vasudev Alwala, Nicolas Carion,Chao-Yuan Wu Ross Girshick, 
              <br>
              Piotr Dollár, Christoph Feichtenhofer
              <br>
              <em>Arxiv</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2408.00714">[paper]</a>
              <!-- <a href="data/2023_zero_shot_qa.bib">bibtex</a> -->
              <a href="https://ai.meta.com/sam2/">[website]</a>
              <a href="https://sam2.metademolab.com/">[demo]</a>
              <a href="https://github.com/facebookresearch/segment-anything-2">[code]</a>
              <p></p>
              <p>
                <font color=3D"crimson"> We present Segment Anything Model 2 (SAM 2 ), a foundation model towards solving promptable visual segmentation in images and videos. 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;""><tbody>
          <tr onmouseout="figure_stop()" onmouseover="figure_start()">
            <td style="padding-left:20px;padding-top:50px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='figure'>
                  <img src='images/mathvision.png' width="160"></div>
                <img src='images/mathvision.png' width="160">
              </div>
              <script type="text/javascript">
                function figure_start() {
                  document.getElementById('figure').style.opacity = "1";
                }

                function figure_stop() {
                  document.getElementById('figure').style.opacity = "0";
                }
                figure_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2307.00716">
                <papertitle>Measuring Multimodal Mathematical Reasoning with the MATH-Vision Dataset
                </papertitle>
              </a>
              <br>
              K. Wang*, <b>J. Pan*</b> and W. Shi* and Z. Lu and M. Zhan and H. Li
              <br>
              <em>NeurIPS</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2307.00716">[paper]</a>
              <!-- <a href="data/2023_zero_shot_qa.bib">bibtex</a> -->
              <a href="https://mathvision-cuhk.github.io/">[website]</a>
              <a href="https://github.com/mathllm/MATH-V">[code]</a>
              <p></p>
              <p>
                <font color=3D"crimson"> Math-Vision (Math-V), a curated collection of 3,040 high-quality mathematical problems with visual contexts sourced from real math competitions that provides a comprehensive and diverse set of challenges for evaluating the mathematical reasoning abilities of LMMs.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;""><tbody>
          <tr onmouseout="figure_stop()" onmouseover="figure_start()">
            <td style="padding-left:20px;padding-top:30px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='figure'>
                  <img src='images/journeydb.png' width="160"></div>
                <img src='images/journeydb.png' width="160">
              </div>
              <script type="text/javascript">
                function figure_start() {
                  document.getElementById('figure').style.opacity = "1";
                }

                function figure_stop() {
                  document.getElementById('figure').style.opacity = "0";
                }
                figure_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2307.00716">
                <papertitle>JourneyDB: A Benchmark for Generative Image Understanding
                </papertitle>
              </a>
              <br>
              <b>J. Pan*</b>, K. Sun*, Y. Ge, H. Li, H. Duan, X. Wu, R. Zhang, A. Zhou, Z. Qin, Y. Wang, J. Dai, Y. Qiao, H. Li
              <br>
              <em>NeurIPS</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2307.00716">[paper]</a>
              <!-- <a href="data/2023_zero_shot_qa.bib">bibtex</a> -->
              <a href="https://github.com/JourneyDB/JourneyDB">[website]</a>
              <p></p>
              <p>
                <font color=3D"crimson"> JourneyDB is a large-scale generated image understanding dataset that contains 4,4M high-resolution generated images, annotated with corresponding text prompt, image caption, and visual question answering.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;""><tbody>
          <tr onmouseout="figure_stop()" onmouseover="figure_start()">
            <td style="padding-left:20px;padding-top:50px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='figure'>
                  <img src='images/zero_shot_qa.png' width="160"></div>
                <img src='images/zero_shot_qa.png' width="160">
              </div>
              <script type="text/javascript">
                function figure_start() {
                  document.getElementById('figure').style.opacity = "1";
                }

                function figure_stop() {
                  document.getElementById('figure').style.opacity = "0";
                }
                figure_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2306.11732">
                <papertitle>Retrieving-to-Answer: Zero-Shot Video Question Answering with Frozen Large Language Models
                </papertitle>
              </a>
              <br>
              <b>J. Pan</b>, Z. Lin, Y. Ge, X. Zhu, R. Zhang, Y. Wang, Y. Qiao, H. Li
              <br>
              <em>ICCVW</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2306.11732">[paper]</a>
              <!-- <a href="data/2023_zero_shot_qa.bib">bibtex</a> -->
              <p></p>
              <p>
                <font color=3D"crimson"> We propose a simple yet effective Retrieving-to-Answer (R2A) framework and achieves SOTA videoQA with zero training.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;""><tbody>
          <tr onmouseout="figure_stop()" onmouseover="figure_start()">
            <td style="padding-left:20px;padding-top:50px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='figure'>
                  <img src='images/st_adapter.png' width="160"></div>
                <img src='images/st_adapter.png' width="160">
              </div>
              <script type="text/javascript">
                function figure_start() {
                  document.getElementById('figure').style.opacity = "1";
                }

                function figure_stop() {
                  document.getElementById('figure').style.opacity = "0";
                }
                figure_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2206.13559.pdf">
                <papertitle>ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning
                </papertitle>
              </a>
              <br>
              <b>J. Pan</b>, Z. Lin, X. Zhu, J. Shao, H. Li
              <br>
              <em>NeurIPS</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2206.13559">[paper]</a>
              <!-- <a href="data/2022_neurips.bib">bibtex</a>,  -->
              <a href="https://github.com/linziyi96/st-adapter">[code]</a>
              <p></p>
              <p>
                <font color=3D"crimson"> We propose a SpatioTemporal Adapter (ST-Adapter) for image-to-video transfer learning.
                  With ~20 times fewer parameters, we achieve on-par or better results compared to full fine-tuning strategy and state-of-theart video models.
            </p>
            </td>
          </tr>
        </tbody></table>
        <!-- <table style="width:10%;margin-left:20px"left"><tbody>
          <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=P8vcCvj_7_Z3QyyGD__x-J1xcoFUrmiib9cPIwnHKZ4"></script>        </tbody></table> -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;""><tbody>
          <tr onmouseout="figure_stop()" onmouseover="figure_start()">
            <td style="padding-left:20px;padding-top:50px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='figure'>
                  <img src='images/edgevit.png' width="160"></div>
                <img src='images/edgevit.png' width="160">
              </div>
              <script type="text/javascript">
                function figure_start() {
                  document.getElementById('figure').style.opacity = "1";
                }

                function figure_stop() {
                  document.getElementById('figure').style.opacity = "0";
                }
                figure_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2205.03436.pdf">
                <papertitle>EdgeViTs: Competing Light-weight CNNs on Mobile Devices with Vision Transformers
                </papertitle>
              </a>
              <br>
              <b>J. Pan</b>, A. Bulat, F. Tan, X. Zhu, L. Dudziak, H. Li, G. Tzimiropoulos, B. Martinez
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2205.03436">[paper]</a>
              <!-- <a href="data/2022_edgevit.bib">bibtex</a>,  -->
              <a href="https://github.com/saic-fi/edgevit">[code]</a>
              <p></p>
              <p>
                We introduce EdgeViTs, a new family of light-weight ViTs that for the first time, enable attention based vision models to compete with
                the best light-weight CNNs in the tradeoff between accuracy and on device efficiency. </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;""><tbody>
          <tr onmouseout="figure_stop()" onmouseover="figure_start()">
            <td style="padding-left:20px;padding-top:50px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='figure'>
                  <img src='images/cvpr21.png' width="160"></div>
                <img src='images/cvpr21.png' width="160">
              </div>
              <script type="text/javascript">
                function figure_start() {
                  document.getElementById('figure').style.opacity = "1";
                }

                function figure_stop() {
                  document.getElementById('figure').style.opacity = "0";
                }
                figure_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2006.07976.pdf">
                <papertitle>Actor-Context-Actor Relation Network forSpatio-Temporal Action Localization
                </papertitle>
              </a>
              <br>
              <b>J. Pan</b>*, S. Chen*, J. Shao, Z. Shou, H. Li
              <br>
              <em>CVPR</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2006.07976">[paper]</a>
              <!-- <a href="data/2021_cvpr.bib">bibtex</a>,  -->
              <a href="https://github.com/Siyu-C/ACAR-Net">[code]</a>
              <p></p>
              <p>
                We propose to explicitly model the Actor-Context-Actor Relation, 
                which is the relation between two actors based on their interactions with the context. Notably, our method ranks first in the AVA-Kinetics action localization task of ActivityNet Challenge 2020, outperforming other entries by a significant margin (+6.71mAP).
               </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;""><tbody>
          <tr onmouseout="figure_stop()" onmouseover="figure_start()">
            <td style="padding-left:20px;padding-top:50px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='figure'>
                  <img src='images/cvpr19.png' width="160"></div>
                <img src='images/cvpr19.png' width="160">
              </div>
              <script type="text/javascript">
                function figure_start() {
                  document.getElementById('figure').style.opacity = "1";
                }

                function figure_stop() {
                  document.getElementById('figure').style.opacity = "0";
                }
                figure_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1903.04480.pdf">
                <papertitle>Video Generation from Single Semantic Label Map
                </papertitle>
              </a>
              <br>
              <b>J. Pan</b>, C. Wang, X. Jia, J. Shao, L. Sheng, J. Yan, X. Wang
              <br>
              <em>CVPR</em>, 2019
              <br>
              <a href="https://arxiv.org/pdf/1903.04480">[paper]</a>
              <!-- <a href="data/2019_cvpr.bib">bibtex</a> -->
              <a href="https://github.com/junting/seg2vid">[code]</a>
              <p></p>
              <p>
                We present a two-stage framework for video synthesis conditioned on a single semantic label map.
                At the first stage, we generate the starting frame from a semantic label map. Then, we propose a
                flow prediction network to transform the initial frame to a video sequence.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;""><tbody>
          <tr onmouseout="figure_stop()" onmouseover="figure_start()">
            <td style="padding-left:20px;padding-top:50px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='figure'>
                  <img src='images/eccv2018.png' width="160"></div>
                <img src='images/eccv2018.png' width="160">
              </div>
              <script type="text/javascript">
                function figure_start() {
                  document.getElementById('figure').style.opacity = "1";
                }

                function figure_stop() {
                  document.getElementById('figure').style.opacity = "0";
                }
                figure_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content_ECCV_2018/html/Zheng_Shou_Online_Detection_of_ECCV_2018_paper.html">
                <papertitle>Online detection of action start in untrimmed, streaming videos
                </papertitle>
              </a>
              <br>
              <b>J. Pan</b>*, Z. Shou*, J. Chan, K. Miyazawa, H. Mansour, A. Vetro, X. Giro-i-Nieto, SF. Chang
              <br>
              <em>ECCV</em>, 2018
              <br>
              <a href="https://openaccess.thecvf.com/content_ECCV_2018/html/Zheng_Shou_Online_Detection_of_ECCV_2018_paper.html">[paper]</a>
              <!-- <a href="data/2018_eccv.bib">bibtex</a> -->
              <!-- <a href="https://github.com/junting/seg2vid">github</a> -->
              <p></p>
              <p>
                We present a novel Online Detection of Action Start task in a practical setting involving untrimmed, unconstrained videos. Three training
                methods have been proposed to specifically improve the capability of ODAS models in detecting action timely and accurately.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;""><tbody>
          <tr onmouseout="figure_stop()" onmouseover="figure_start()">
            <td style="padding-left:20px;padding-top:50px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='figure'>
                  <img src='images/cvpr2016.png' width="160"></div>
                <img src='images/cvpr2016.png' width="160">
              </div>
              <script type="text/javascript">
                function figure_start() {
                  document.getElementById('figure').style.opacity = "1";
                }

                function figure_stop() {
                  document.getElementById('figure').style.opacity = "0";
                }
                figure_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Pan_Shallow_and_Deep_CVPR_2016_paper.pdf">
                <papertitle>Shallow and Deep Convolutional Networks for Saliency Prediction
                </papertitle>
              </a>
              <br>
              <b>J. Pan</b>*, K. McGuinness*, NE. O'Connor, E. Sayrol, X. Giro-i-Nieto          <br>
              <em>CVPR</em>, 2016
              <br>
              <a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Pan_Shallow_and_Deep_CVPR_2016_paper.pdf">[paper]</a>
              <!-- <a href="data/2016_cvpr.bib">bibtex</a> -->
              <a href="https://github.com/imatge-upc/saliency-2016-cvpr">[code]</a>
              <p></p>
              <p>
               We present the first end-to-end CNN based saliency prediction network.</p>
            </td>
          </tr>
        </tbody></table>


      <table style="width:10%;margin-left:20px"left"><tbody>
        <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=fHKOjyAQM5OZpf7_YENiKNVtE2RzjQAC3yRTLPmVxuY"></script>
      </tbody></table>



  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                cr: <a href="https://github.com/jonbarron/jonbarron_website">J.Baron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
  
</body>
</html>
